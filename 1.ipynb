{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train_data = pd.read_csv('A3 files/train_data.csv')\n",
    "X = train_data['Sentence']\n",
    "Y = train_data['Transformed sentence'].apply(lambda x: '<'+x)\n",
    "\n",
    "ch2_idx = {char: idx for idx, char in enumerate(set(char for string in X for char in string))}\n",
    "ch2_idx.update({'<': len(ch2_idx)})\n",
    "\n",
    "X = [[ch2_idx[char] for char in string] for string in X]\n",
    "Y = [[ch2_idx[char] for char in string] for string in Y]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, train_size=0.8)\n",
    "trainset = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
    "valset = TensorDataset(torch.LongTensor(x_valid), torch.LongTensor(y_valid))\n",
    "max_len = max(len(i) for i in x_train)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_k = d_model // nhead\n",
    "\n",
    "        self.W_q = nn.Linear(d_model ,d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        return x.view(batch_size, seq_length, self.nhead, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        encoding = torch.zeros(max_len, d_model)\n",
    "\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('encoding', encoding.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x, enc_output):\n",
    "        attn_output = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, d_ff, max_len, rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, nhead, d_ff, rate) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, nhead, d_ff, rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        x = self.dropout(self.encoding(self.embedding(src)))\n",
    "        y = self.dropout(self.encoding(self.embedding(tgt)))\n",
    "        \n",
    "        enc_output = x\n",
    "        for en_layer in self.encoder:\n",
    "            enc_output = en_layer(enc_output)\n",
    "\n",
    "        dec_output = y\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_output = dec_layer(dec_output, enc_output)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, loss_func, optimizer, vocab_size, trainloader, valloader):\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for (src, tgt) in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = loss_func(output.contiguous().view(-1, vocab_size), tgt[:,1:].contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        t_loss, t_acc = evaluate(model, loss_func, vocab_size, trainloader)\n",
    "        v_loss, v_acc = evaluate(model, loss_func, vocab_size, valloader)\n",
    "        print('Epoch: {}/{} Train [Loss: {:.4f} Accuracy: {:.4f}] Validation [Loss: {:.4f} Accuracy: {:.4f}]'.format(1+_, n_epochs, t_loss, t_acc, v_loss, v_acc))\n",
    "\n",
    "def evaluate(model, loss_func, vocab_size, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t_loss, correct, total = 0, 0, 0\n",
    "        for (src, tgt) in dataloader:\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            loss = loss_func(output.contiguous().view(-1, vocab_size), tgt[:,1:].contiguous().view(-1))\n",
    "            t_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, dim=2)\n",
    "            correct += (predicted == tgt[:,1:]).sum().item()\n",
    "            total += predicted.shape[0]\n",
    "    avg_loss = t_loss / len(dataloader)\n",
    "    accuracy = correct / (total * (tgt.size(1)-1))\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Train [Loss: 1.3399 Accuracy: 0.5853] Validation [Loss: 1.3659 Accuracy: 0.5710]\n",
      "Epoch: 2/10 Train [Loss: 0.2870 Accuracy: 0.9119] Validation [Loss: 0.3017 Accuracy: 0.9052]\n",
      "Epoch: 3/10 Train [Loss: 0.1092 Accuracy: 0.9452] Validation [Loss: 0.1261 Accuracy: 0.9357]\n",
      "Epoch: 4/10 Train [Loss: 0.0960 Accuracy: 0.9466] Validation [Loss: 0.1048 Accuracy: 0.9396]\n",
      "Epoch: 5/10 Train [Loss: 0.0928 Accuracy: 0.9463] Validation [Loss: 0.1031 Accuracy: 0.9375]\n",
      "Epoch: 6/10 Train [Loss: 0.0848 Accuracy: 0.9499] Validation [Loss: 0.0985 Accuracy: 0.9351]\n",
      "Epoch: 7/10 Train [Loss: 0.0845 Accuracy: 0.9491] Validation [Loss: 0.0954 Accuracy: 0.9376]\n",
      "Epoch: 8/10 Train [Loss: 0.0905 Accuracy: 0.9477] Validation [Loss: 0.1046 Accuracy: 0.9376]\n",
      "Epoch: 9/10 Train [Loss: 0.0819 Accuracy: 0.9537] Validation [Loss: 0.0969 Accuracy: 0.9360]\n",
      "Epoch: 10/10 Train [Loss: 0.0823 Accuracy: 0.9533] Validation [Loss: 0.1006 Accuracy: 0.9372]\n"
     ]
    }
   ],
   "source": [
    "nhead = 8\n",
    "d_ff = 2048 # dimension of inner fully connected layer\n",
    "d_model = 512 # dimension of model sub-layers' outputs\n",
    "num_layers = 6\n",
    "vocab_size = 28\n",
    "dropout_rate = 0.2\n",
    "learn_rate = 0.0001\n",
    "\n",
    "model = Transformer(vocab_size, d_model, nhead, num_layers, d_ff, max_len, dropout_rate)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "n_epochs = 10\n",
    "train(model, n_epochs, loss_func, optimizer, vocab_size, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.08244324340061708, 0.9533482142857143)\n",
      "(0.10064782947301865, 0.9372321428571428)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, loss_func, vocab_size, trainloader))\n",
    "print(evaluate(model, loss_func, vocab_size, valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(pred, true):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "    return correct\n",
    "\n",
    "def predict(model, idx2_ch, ch2_idx, dataloader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (src, _) in dataloader:\n",
    "            max_len = src.size(1)\n",
    "            pred = torch.zeros(1, max_len, dtype=torch.long)\n",
    "            pred[0][0] = ch2_idx['<']\n",
    "            for i in range(max_len):\n",
    "                output = model(src, pred)\n",
    "                output = torch.argmax(output, dim=-1).tolist()[0]\n",
    "                pred[0, i] = output[i]\n",
    "                # print(output, pred)\n",
    "            output = pred[0].tolist()\n",
    "            result = [idx2_ch[idx] for idx in output]\n",
    "            # print(result)\n",
    "            y_pred.append(''.join(result))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5260695640041958, 0.909125)\n",
      "+--------+----------+\n",
      "| Length | #Correct |\n",
      "+--------+----------+\n",
      "|   0    |   996    |\n",
      "|   1    |   816    |\n",
      "|   2    |   169    |\n",
      "|   3    |    16    |\n",
      "|   4    |    3     |\n",
      "|   5    |    0     |\n",
      "|   6    |    0     |\n",
      "|   7    |    0     |\n",
      "|   8    |    0     |\n",
      "+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('A3 files/eval_data.csv')\n",
    "X = test_data['Sentence']\n",
    "Y = test_data['Transformed sentence'].apply(lambda x: '<'+x)\n",
    "\n",
    "ch2_idx = {char: idx for idx, char in enumerate(set(char for string in X for char in string))}\n",
    "ch2_idx.update({'<': len(ch2_idx)})\n",
    "idx2_ch = {idx: char for char, idx in ch2_idx.items()}\n",
    "\n",
    "x_test = [[ch2_idx[char] for char in string] for string in X]\n",
    "y_test = [[ch2_idx[char] for char in string] for string in Y]\n",
    "\n",
    "testset = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "print(evaluate(model, loss_func, vocab_size, testloader))\n",
    "predicted = predict(model, idx2_ch, ch2_idx, testloader)\n",
    "\n",
    "freq = {}\n",
    "for i in range(max_len+1):\n",
    "    freq[i] = 0\n",
    "\n",
    "# print('Original\\tPredicted')\n",
    "for (y_true, y_pred) in zip(Y, predicted):\n",
    "    num = check(y_true[1:], y_pred)\n",
    "    freq[num] += 1\n",
    "    # print('{}\\t{}\\t{}'.format(y_true[1:], y_pred, num))\n",
    "\n",
    "t = PrettyTable(['Length', '#Correct'])\n",
    "for i in range(max_len+1):\n",
    "    t.add_row([i, freq[i]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| Length | #Correct |\n",
      "+--------+----------+\n",
      "|   0    |   4088   |\n",
      "|   1    |   1320   |\n",
      "|   2    |   180    |\n",
      "|   3    |    11    |\n",
      "|   4    |    1     |\n",
      "|   5    |    0     |\n",
      "|   6    |    0     |\n",
      "|   7    |    0     |\n",
      "|   8    |    0     |\n",
      "+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "Y = train_data['Transformed sentence'].apply(lambda x: '<'+x)\n",
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "predicted = predict(model, idx2_ch, ch2_idx, trainloader)\n",
    "\n",
    "freq = {}\n",
    "for i in range(max_len+1):\n",
    "    freq[i] = 0\n",
    "\n",
    "# print('Original\\tPredicted')\n",
    "for (y_true, y_pred) in zip(Y, predicted):\n",
    "    num = check(y_true[1:], y_pred)\n",
    "    freq[num] += 1\n",
    "    # print('{}\\t{}\\t{}'.format(y_true[1:], y_pred, num))\n",
    "\n",
    "t = PrettyTable(['Length', '#Correct'])\n",
    "for i in range(max_len+1):\n",
    "    t.add_row([i, freq[i]])\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
